#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Based on Steve Androulakis's
# proof of concept that produces a virtual FUSE-mountable
# file/directory structure from a python dictionary
# By Steve Androulakis <http://github.com/steveandroulakis>
# Requires FUSE (linux: use package manager to install)
# Requires python-fuse pip install fuse-python
# USE:
# mkdir MyTardis
# Mount: mytardis_mount MyTardis -f
# Unmount: fusermount -uz MyTardis

# To Do: Make sure file/directory names are legal, e.g. they shouldn't contain the '/' character.

# To Do: Ensure that subdirectories within datasets are working properly, including intermediate subdirs, not containing datafiles.

# To Do: Improve efficiency / response time, e.g. re-use file object between subsequent reads within same datafile.

# To Do: Tidy up code and make sure it refreshes directories when necessary.

import fuse
import stat
import time
import requests
import os
import sys
import getpass
import subprocess
import logging
import threading
import ast
import errno
from af_unix_socket_client import MyTardisDatafileDescriptor

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
# We don't really want to log to STDOUT.  We assume that this script
# will be called with STDOUT redirected to a file.
streamHandler = logging.StreamHandler(sys.stdout)
streamHandler.setLevel(logging.DEBUG)
log_format_string = '%(asctime)s - %(name)s - %(module)s - %(funcName)s - %(lineno)d - %(levelname)s - %(message)s'
streamHandler.setFormatter(logging.Formatter(log_format_string))
logger.addHandler(streamHandler)

if len(sys.argv)<2:
    logger.debug("Usage: mytardis_mount MOUNT_DIR [-f]")
    sys.exit(1)

fuseMountDir = os.path.expanduser(sys.argv[1])
if not os.path.exists(fuseMountDir):
    os.makedirs(fuseMountDir)

myTardisUsername = getpass.getuser()
proc = subprocess.Popen(["sudo","-u","mytardis","my_api_key"], stdout=subprocess.PIPE)
myTardisApiKey = proc.stdout.read().strip()

proc = subprocess.Popen(["id","-u"], stdout=subprocess.PIPE)
uid = proc.stdout.read().strip()

proc = subprocess.Popen(["id","-g"], stdout=subprocess.PIPE)
gid = proc.stdout.read().strip()

myTardisUrl = "https://mytardis.massive.org.au"
headers={'Authorization':'ApiKey ' + myTardisUsername + ":" + myTardisApiKey}

DEBUG = False

fuse.fuse_python_api = (0, 2)
 
# Use same timestamp for all files
_file_timestamp = int(time.time())
 
# FILES[directory/file/path] = (size_in_bytes, is_directory)
FILES = dict()
DATAFILE_IDS = dict()
DATAFILE_SIZES = dict()
DATAFILE_FILE_OBJECTS = dict()
DATAFILE_CLOSE_TIMERS = dict()
 
url = myTardisUrl + "/api/v1/experiment/?format=json&limit=0"
logger.debug(url)
response = requests.get(url=url,headers=headers)
expRecordsJson = response.json()
numExpRecordsFound = expRecordsJson['meta']['total_count']
logger.debug(str(numExpRecordsFound) + " experiment record(s) found for user " + myTardisUsername)
for expRecordJson in expRecordsJson['objects']:
    expDirName = str(expRecordJson['id']) + " - " + expRecordJson['title'].encode('ascii','ignore')
    FILES['/' + expDirName] = (0, True)

def file_array_to_list(files):
    # files need to be returned in this format
    #FILES = [('file1', 15, False), ('file2', 15, False), ('directory', 15, True)]
 
    l = list()
    for key, val in files.iteritems():
        l.append((file_from_key(key), val[0], val[1]))
 
    return l
 
 
def file_from_key(key):
    return key.rsplit("/")[-1]
 
 
class MyStat(fuse.Stat):
    """
    Convenient class for Stat objects.
    Set up the stat object with appropriate
    values depending on constructor args.
    """
    def __init__(self, is_dir, size):
        fuse.Stat.__init__(self)
        if is_dir:
            self.st_mode = stat.S_IFDIR|stat.S_IRUSR|stat.S_IXUSR
            self.st_nlink = 2
        else:
            self.st_mode = stat.S_IFREG|stat.S_IRUSR
            self.st_nlink = 1
            self.st_size = size
        self.st_atime = _file_timestamp
        self.st_mtime = _file_timestamp
        self.st_ctime = _file_timestamp
 
        #self.st_uid = 1001 # id -u
        self.st_uid = int(uid)
        #self.st_gid = 1001 # id -g
        self.st_gid = int(gid)
 
class MyFS(fuse.Fuse):
    def __init__(self, *args, **kw):
        fuse.Fuse.__init__(self, *args, **kw)
 
    def getattr(self, path):
        path = path.rstrip("*")
        if path != "/":
            path = path.rstrip("/")
        if DEBUG:
            logger.debug("^ getattr: path = " + path)
 

        #size = entry.text and len(entry.text.strip()) or 0
        #IsDir
        if path == "/" or path == "/*":
            #return MyStat(True, 4096)
            return MyStat(True, 0)
        else:
            try:
                return MyStat(FILES[path][1], FILES[path][0])
            except KeyError:
                return -errno.ENOENT
                #return MyStat(True, 0)
 
    def getdir(self, path):
        if DEBUG:
            logger.debug('getdir called:', path)
        return file_array_to_list(FILES)
 
    def readdir(self, path, offset):
        if DEBUG:
            logger.debug("^ readdir: path = \"" + path + "\"")
 
        for e in '.', '..':
            yield fuse.Direntry(e)
 
        pathComponents = path.split("/",3)
        if len(pathComponents)>1 and pathComponents[1]!='':
            expDirName = pathComponents[1]
            experimentId = expDirName.split(" ")[0]
        if len(pathComponents)>2 and pathComponents[2]!='':
            datasetDirName = pathComponents[2]
            datasetId    = datasetDirName.split(" ")[0]
        if len(pathComponents)>3 and pathComponents[3]!='':
            subdirectory = pathComponents[3] # NOT USED IN THIS METHOD. SHOULD IT BE?

        if len(pathComponents)==1:
            url = myTardisUrl + "/api/v1/experiment/?format=json&limit=0"
            logger.debug(url)
            response = requests.get(url=url,headers=headers)
            if response.status_code<200 or response.status_code>=300:
                logger.debug(url)
                logger.debug("Response status_code = " + str(response.status_code))
            expRecordsJson = response.json()
            numExpRecordsFound = expRecordsJson['meta']['total_count']
            logger.debug(str(numExpRecordsFound) + " experiment record(s) found for user " + myTardisUsername)

            # Doesn't check for deleted experiments - only adds to FILES dictionary.
            for expRecordJson in expRecordsJson['objects']:
                expDirName = str(expRecordJson['id']) + " - " + expRecordJson['title'].encode('ascii','ignore')
                FILES['/' + expDirName] = (0, True)

        if len(pathComponents)==2 and pathComponents[1]!='':
            url = myTardisUrl + "/api/v1/dataset/?format=json&limit=0&experiments__id=" + experimentId
            logger.debug(url)
            response = requests.get(url=url,headers=headers)
            if response.status_code<200 or response.status_code>=300:
                logger.debug("Response status_code = " + str(response.status_code))
            datasetRecordsJson = response.json()
            numDatasetRecordsFound = datasetRecordsJson['meta']['total_count']
            logger.debug(str(numDatasetRecordsFound) + " dataset record(s) found for exp ID " + experimentId)

            for datasetJson in datasetRecordsJson['objects']:
                datasetDirName = str(datasetJson['id']) + " - " + datasetJson['description'].encode('ascii','ignore')
                FILES['/' + expDirName + '/' + datasetDirName] = (0, True)

        if len(pathComponents)==3 and pathComponents[1]!='':
            FILES['/' + expDirName + '/' + datasetDirName] = (0, True)
            DATAFILE_IDS[datasetId] = dict()
            DATAFILE_SIZES[datasetId] = dict()
            DATAFILE_FILE_OBJECTS[datasetId] = dict()
            DATAFILE_CLOSE_TIMERS[datasetId] = dict()

            useApi = False

            if useApi:
                url = myTardisUrl + "/api/v1/dataset_file/?format=json&limit=0&dataset__id=" + str(datasetId)
                logger.debug(url)
                response = requests.get(url=url,headers=headers)
                datafileRecordsJson = response.json()
                numDatafileRecordsFound = datafileRecordsJson['meta']['total_count']
            else:
                cmd = ['sudo','-u','mytardis','/usr/local/bin/get_dataset_datafiles',experimentId,datasetId]
                logger.debug(str(cmd))
                proc = subprocess.Popen(cmd,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
                stdout,stderr = proc.communicate()
                if stderr!=None and stderr!="":
                    logger.debug(stderr)

                datafileDictsString = stdout.strip()
                #logger.debug("datafileDictsString: " + datafileDictsString)
                datafileDicts = ast.literal_eval(datafileDictsString)
                numDatafileRecordsFound = len(datafileDicts)

            logger.debug(str(numDatafileRecordsFound) + " datafile record(s) found for dataset ID " + str(datasetId))
        
            if useApi:
                datafileDicts =  datafileRecordsJson['objects']

            for datafileDict in datafileDicts:
                #logger.debug(str(datafileDict))
                datafileId = datafileDict['id']
                if useApi:
                    datafileDirectory = datafileDict['directory'].encode('ascii','ignore').strip('/')
                else:
                    datafileDirectory = datafileDict['directory']
                    if datafileDirectory is None:
                        datafileDirectory = ""
                    else:
                        datafileDirectory = datafileDirectory.encode('ascii','ignore').strip('/')
                datafileName = datafileDict['filename'].encode('ascii','ignore')
                datafileSize = int(datafileDict['size'].encode('ascii','ignore'))
                if datafileDirectory!="":
                    # I think we really need to ensure that intermediate directories are created too
                    # which is not done yet, e.g. if a dataset contains subdir1/subdir2/datafile1.txt,
                    # and there are no datafiles directly in subdir1, only the directory subdir2,
                    # then we need to ensure that subdir1 is registered in our FILES dictionary.

                    # Intermediate subdirectories
                    for i in reversed(range(1,len(datafileDirectory.split('/')))):
                        intermediateSubdirectory = datafileDirectory.rsplit('/',i)[0]
                        FILES['/' + expDirName + '/' + datasetDirName + '/' + intermediateSubdirectory] = (0, True)

      
                    FILES['/' + expDirName + '/' + datasetDirName + '/' + datafileDirectory] = (0, True)
                    FILES['/' + expDirName + '/' + datasetDirName + '/' + datafileDirectory + '/' + datafileName] = (datafileSize, False)
                else:
                    FILES['/' + expDirName + '/' + datasetDirName + '/' + datafileName] = (datafileSize, False)
                if datafileDirectory not in DATAFILE_IDS[datasetId]:
                    DATAFILE_IDS[datasetId][datafileDirectory] = dict()
                DATAFILE_IDS[datasetId][datafileDirectory][datafileName] = datafileId
                if datafileDirectory not in DATAFILE_SIZES[datasetId]:
                    DATAFILE_SIZES[datasetId][datafileDirectory] = dict()
                DATAFILE_SIZES[datasetId][datafileDirectory][datafileName] = datafileSize
                if datafileDirectory not in DATAFILE_FILE_OBJECTS[datasetId]:
                    DATAFILE_FILE_OBJECTS[datasetId][datafileDirectory] = dict()
                DATAFILE_FILE_OBJECTS[datasetId][datafileDirectory][datafileName] = None
                if datafileDirectory not in DATAFILE_CLOSE_TIMERS[datasetId]:
                    DATAFILE_CLOSE_TIMERS[datasetId][datafileDirectory] = dict()
                DATAFILE_CLOSE_TIMERS[datasetId][datafileDirectory][datafileName] = None

        path_depth = path.count('/')
        for key, val in FILES.iteritems():
            key_depth = key.count('/')

            if path == "/":
                path_depth = 0

            if key.startswith(path) and key_depth == path_depth + 1:
                yield(fuse.Direntry(file_from_key(key)))

    def read(self, path, leng, offset):

        if DEBUG:
            logger.debug("read(...) path = " + path)

        filename     = path.rsplit(os.sep)[-1]
        pathComponents = path.split(os.sep,3)
        experimentId = pathComponents[1].split(" ")[0]
        datasetId    = pathComponents[2].split(" ")[0]
        if os.sep in pathComponents[3]:
            subdirectory = pathComponents[3].rsplit(os.sep,1)[0]
        else:
            subdirectory = ""
        if DEBUG:
            logger.debug("read request for %s with length %d and offset %d" % \
                (filename, leng, offset))

        datafileId = DATAFILE_IDS[datasetId][subdirectory][filename]

        datafileSize = DATAFILE_SIZES[datasetId][subdirectory][filename]
        if DEBUG:
            logger.debug("datafileSize is " + str(datafileSize))

        if DATAFILE_FILE_OBJECTS[datasetId][subdirectory][filename] is not None:
            # Found a file object to reuse, so let's reset the timer for closing the file:
            fileObject = DATAFILE_FILE_OBJECTS[datasetId][subdirectory][filename] 
            DATAFILE_CLOSE_TIMERS[datasetId][subdirectory][filename].cancel()
            def closeFile(fileObj, dictObj, key):
                fileObj.close()
                dictObj[key] = None
            DATAFILE_CLOSE_TIMERS[datasetId][subdirectory][filename] = threading.Timer(30.0, closeFile, [fileObject, DATAFILE_FILE_OBJECTS[datasetId][subdirectory], filename])
            DATAFILE_CLOSE_TIMERS[datasetId][subdirectory][filename].start()
        else:
            myTardisDatafileDescriptor = MyTardisDatafileDescriptor.get_file_descriptor(experimentId, datafileId)
            fileDescriptor = None
            if DEBUG:
                logger.debug("Message: " + myTardisDatafileDescriptor.message)
            if myTardisDatafileDescriptor.fileDescriptor is not None:
                fileDescriptor = myTardisDatafileDescriptor.fileDescriptor
            else:
                logger.debug("myTardisDatafileDescriptor.fileDescriptor is None.")

            fileObject = os.fdopen(fileDescriptor)
            DATAFILE_FILE_OBJECTS[datasetId][subdirectory][filename] = fileObject
            # Schedule file to be closed in 30 seconds, unless it is used before
            # then, in which case the timer will be reset.
            def closeFile(fileObj, dictObj, key):
                fileObj.close()
                dictObj[key] = None
            DATAFILE_CLOSE_TIMERS[datasetId][subdirectory][filename] = threading.Timer(30.0, closeFile, [fileObject, DATAFILE_FILE_OBJECTS[datasetId][subdirectory], filename])
            DATAFILE_CLOSE_TIMERS[datasetId][subdirectory][filename].start()

        fileObject.seek(offset)
        data = fileObject.read(leng)

        return data
 
if __name__ == '__main__':
    fs = MyFS()
    fs.parse(errex=1)
    fs.main()

